{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EGjIAR88CxOI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "from utils.siren import SIREN\n",
        "from utils.metrics import TVLoss\n",
        "from scipy.spatial.transform import Rotation\n",
        "from PIL import Image\n",
        "from pytorch3d.renderer.mesh import rasterize_meshes\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.ops import interpolate_face_attributes\n",
        "from pytorch3d.structures import Meshes\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQu-iKT8pWf",
        "outputId": "aafd99fb-f335-4095-aad5-e6314fec9c6d"
      },
      "outputs": [],
      "source": [
        "def image_to_tensor(img_path, unsqueeze=True):\n",
        "    rgb = transforms.ToTensor()(Image.open(img_path))\n",
        "    if unsqueeze:\n",
        "        rgb = rgb.unsqueeze(0)\n",
        "    return rgb\n",
        "\n",
        "def load_images(view0_path, view1_path, view2_path): # future TODO: change to dataloader\n",
        "    # img0 = cv2.imread(view0_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # img1 = cv2.imread(view1_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # img2 = cv2.imread(view2_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img0 = cv2.imread(view0_path)\n",
        "    img1 = cv2.imread(view1_path)\n",
        "    img2 = cv2.imread(view2_path)\n",
        "    img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
        "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    img0 = (img0 - 127.5) / 127.5\n",
        "    img1 = (img1 - 127.5) / 127.5\n",
        "    img2 = (img2 - 127.5) / 127.5\n",
        "\n",
        "    return torch.from_numpy(img0).float(), torch.from_numpy(img1).float(), torch.from_numpy(img2).float()\n",
        "\n",
        "def load_images(v0path, v1path, v2path):\n",
        "    img0 = image_to_tensor(v0path)\n",
        "    img1 = image_to_tensor(v1path)\n",
        "    img2 = image_to_tensor(v2path)\n",
        "    return img0, img1, img2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### estimate inverse depth and warp from depth linearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_camera_matrices(width, height, focal_length_m, sensor_width_m,\n",
        "                       location=[0,0,0], rotation=[0,0,0]):\n",
        "\n",
        "    # 内部パラメータ\n",
        "    focal_px = (width * focal_length_m) / sensor_width_m\n",
        "    K = np.array([\n",
        "        [focal_px, 0, width/2],\n",
        "        [0, focal_px, height/2],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    # 外部パラメータ\n",
        "    rotation_rad = np.array(rotation) * np.pi / 180\n",
        "    R = Rotation.from_euler('xyz', rotation_rad).as_matrix()\n",
        "    t = np.array(location).reshape(3, 1)\n",
        "\n",
        "    RT = np.hstack([R, t])\n",
        "\n",
        "    return K, RT\n",
        "def warp_with_projection(image, depth, K_ref, K_tar, RT_ref_to_tar, device='cuda'):\n",
        "    \"\"\"\n",
        "    投影ベースの画像ワーピング\n",
        "    \"\"\"\n",
        "    B, C, H, W = image.shape\n",
        "\n",
        "    # ピクセル座標グリッドの生成\n",
        "    y = torch.arange(H).to(device)\n",
        "    x = torch.arange(W).to(device)\n",
        "    y, x = torch.meshgrid(y, x, indexing='ij')\n",
        "    pixels = torch.stack([x, y, torch.ones_like(x)], dim=0).float()  # (3,H,W)\n",
        "    pixels = pixels.unsqueeze(0).repeat(B,1,1,1)  # (B,3,H,W)\n",
        "    image = image.to(device)\n",
        "    depth = depth.to(device)\n",
        "\n",
        "    # カメラ行列をテンソルに変換\n",
        "    K_ref = torch.tensor(K_ref, dtype=torch.float32).to(device)\n",
        "    K_tar = torch.tensor(K_tar, dtype=torch.float32).to(device)\n",
        "    RT_ref_to_tar = torch.tensor(RT_ref_to_tar, dtype=torch.float32).to(device)\n",
        "\n",
        "    # 3D点への逆投影\n",
        "    inv_K_ref = torch.inverse(K_ref)\n",
        "    rays = torch.matmul(inv_K_ref, pixels.reshape(B,3,-1))  # (B,3,H*W)\n",
        "    points_3d = rays * depth.reshape(B,1,-1)  # (B,3,H*W)\n",
        "\n",
        "    # 同次座標に変換\n",
        "    points_4d = torch.cat([points_3d,\n",
        "                          torch.ones((B,1,H*W), device=device)],\n",
        "                          dim=1)  # (B,4,H*W)\n",
        "\n",
        "    # ターゲット視点への変換\n",
        "    points_tar = torch.matmul(RT_ref_to_tar, points_4d)  # (B,3,H*W)\n",
        "\n",
        "    # ターゲット視点での投影\n",
        "    points_2d_homo = torch.matmul(K_tar, points_tar)  # (B,3,H*W)\n",
        "    points_2d = points_2d_homo[:,:2] / (points_2d_homo[:,2:] + 1e-7)  # (B,2,H*W)\n",
        "    # # 正規化座標系に変換 (-1,1)\n",
        "\n",
        "    points_2d = points_2d.reshape(B,2,H,W)\n",
        "    normalized_points = torch.stack([\n",
        "        2 * points_2d[:,0] / (W-1) - 1,\n",
        "        2 * points_2d[:,1] / (H-1) - 1\n",
        "    ], dim=-1)\n",
        "\n",
        "    # 画像のワーピング\n",
        "    warped = F.grid_sample(image, normalized_points,\n",
        "                          mode='bilinear',\n",
        "                          padding_mode='zeros',\n",
        "                          align_corners=True)\n",
        "\n",
        "    # 有効マスクの生成\n",
        "    valid_mask = (normalized_points.abs() <= 1).all(dim=-1, keepdim=True)\n",
        "    valid_mask = valid_mask & (points_tar[:,2:3] > 0).reshape(B,1,H,W)\n",
        "\n",
        "    return warped, valid_mask\n",
        "\n",
        "\n",
        "def compute_relative_transform(RT1, RT2):\n",
        "    \"\"\"2つの変換行列間の相対変換を計算\"\"\"\n",
        "    RT1_4x4 = np.eye(4)\n",
        "    RT1_4x4[:3] = RT1\n",
        "    RT2_4x4 = np.eye(4)\n",
        "    RT2_4x4[:3] = RT2\n",
        "\n",
        "    relative = RT2_4x4 @ np.linalg.inv(RT1_4x4)\n",
        "    return relative[:3]\n",
        "\n",
        "def warp_with_inverse_depth_projection(image, inverse_depth, device, K, RT_ref_to_view):\n",
        "    \"\"\"\n",
        "    スケーリングを改善したワーピング関数\n",
        "    \"\"\"\n",
        "    # 入力テンソルの形状を統一\n",
        "    if len(image.shape) == 2:\n",
        "        image = image.unsqueeze(0).unsqueeze(0)\n",
        "    if len(inverse_depth.shape) == 2:\n",
        "        inverse_depth = inverse_depth.unsqueeze(0)\n",
        "\n",
        "    h, w = image.shape[-2:]\n",
        "\n",
        "    safety_factor = 1e-4\n",
        "    depth = 1 / inverse_depth + safety_factor\n",
        "    depth = inverse_depth\n",
        "\n",
        "    # 相対変換の計算\n",
        "\n",
        "    warped_image, mask = warp_with_projection(\n",
        "            image, depth, K, K, RT_ref_to_view, device)\n",
        "\n",
        "    return warped_image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### estimate inverse depth and warp by PyTorch3D libraries mesh_renderer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_screen_pixel_coord(h, w):\n",
        "    '''\n",
        "    get normalized pixel coordinates on the screen\n",
        "    x to left, y to down\n",
        "    \n",
        "    e.g.\n",
        "    [0,0][1,0][2,0]\n",
        "    [0,1][1,1][2,1]\n",
        "    output:\n",
        "        pixel_coord: [1,h,w,2]\n",
        "    '''\n",
        "    x = torch.arange(w).to(device)  # [w]\n",
        "    y = torch.arange(h).to(device)  # [h]\n",
        "    x = (x + 0.5) / w\n",
        "    y = (y + 0.5) / h\n",
        "    x = x[None, None, ..., None].repeat(1, h, 1, 1)  # [1,h,w,1]\n",
        "    y = y[None, ..., None, None].repeat(1, 1, w, 1)  # [1,h,w,1]\n",
        "    pixel_coord = torch.cat([x, y], dim=-1)  # [1,h,w,2]\n",
        "    return pixel_coord\n",
        "\n",
        "def lift_to_homo(coord):\n",
        "    '''\n",
        "    return the homo version of coord\n",
        "    input: coord [..., k]\n",
        "    output: homo_coord [...,k+1]\n",
        "    '''\n",
        "    ones = torch.ones_like(coord[..., -1:])\n",
        "    return torch.cat([coord, ones], dim=-1)\n",
        "\n",
        "def get_faces(h, w):\n",
        "    '''\n",
        "    get face connect information\n",
        "    x to left, y to down\n",
        "    e.g.\n",
        "    [0,0][1,0][2,0]\n",
        "    [0,1][1,1][2,1]\n",
        "    faces: [1,nface,3]\n",
        "    '''\n",
        "    x = torch.arange(w - 1).to(device)  # [w-1]\n",
        "    y = torch.arange(h - 1).to(device)  # [h-1]\n",
        "    x = x[None, None, ..., None].repeat(1, h - 1, 1, 1)  # [1,h-1,w-1,1]\n",
        "    y = y[None, ..., None, None].repeat(1, 1, w - 1, 1)  # [1,h-1,w-1,1]\n",
        "\n",
        "    tl = y * w + x\n",
        "    tr = y * w + x + 1\n",
        "    bl = (y + 1) * w + x\n",
        "    br = (y + 1) * w + x + 1\n",
        "\n",
        "    faces_l = torch.cat([tl, bl, br], dim=-1).reshape(1, -1, 3)  # [1,(h-1)(w-1),3]\n",
        "    faces_r = torch.cat([br, tr, tl], dim=-1).reshape(1, -1, 3)  # [1,(h-1)(w-1),3]\n",
        "\n",
        "    return torch.cat([faces_l, faces_r], dim=1)  # [1,nface,3]\n",
        "\n",
        "def get_visible_mask(disparity, beta=10, alpha_threshold=0.3):\n",
        "    '''\n",
        "    filter the disparity map using sobel kernel, then mask out the edge (depth discontinuity)\n",
        "    input:\n",
        "        disparity: [b,h,w,1]\n",
        "    \n",
        "    output:\n",
        "        vis_mask: [b,h,w,1]\n",
        "    '''\n",
        "    b, h, w, _ = disparity.size()\n",
        "    disparity = disparity.reshape(b, 1, h, w)  # [b,1,h,w]\n",
        "    kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "    sobel_x = F.conv2d(disparity, kernel_x, padding=(1, 1))  # [b,1,h,w]\n",
        "    sobel_y = F.conv2d(disparity, kernel_y, padding=(1, 1))  # [b,1,h,w]\n",
        "    sobel_mag = torch.sqrt(sobel_x ** 2 + sobel_y ** 2).reshape(b, h, w, 1)  # [b,h,w,1]\n",
        "    alpha = torch.exp(-1.0 * beta * sobel_mag)  # [b,h,w,1]\n",
        "    vis_mask = torch.greater(alpha, alpha_threshold).float()\n",
        "    return vis_mask\n",
        "\n",
        "def get_perspective_from_intrinsic(cam_int, near, far):\n",
        "        '''\n",
        "        input:\n",
        "            cam_int: [b,3,3]\n",
        "        \n",
        "        output:\n",
        "            persp: [b,4,4]\n",
        "        '''\n",
        "        fx, fy = cam_int[:, 0, 0], cam_int[:, 1, 1]  # [b]\n",
        "        cx, cy = cam_int[:, 0, 2], cam_int[:, 1, 2]  # [b]\n",
        "\n",
        "        one = torch.ones_like(cx)  # [b]\n",
        "        zero = torch.zeros_like(cx)  # [b]\n",
        "\n",
        "        near_z, far_z = near * one, far * one\n",
        "        a = (near_z + far_z) / (far_z - near_z)\n",
        "        b = -2.0 * near_z * far_z / (far_z - near_z)\n",
        "\n",
        "        matrix = [[2.0 * fx, zero, 2.0 * cx - 1.0, zero],\n",
        "                  [zero, 2.0 * fy, 2.0 * cy - 1.0, zero],\n",
        "                  [zero, zero, a, b],\n",
        "                  [zero, zero, one, zero]]\n",
        "        # -> [[b,4],[b,4],[b,4],[b,4]] -> [b,4,4]        \n",
        "        persp = torch.stack([torch.stack(row, dim=-1) for row in matrix], dim=-2)  # [b,4,4]\n",
        "        return persp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "eps = 1e-1\n",
        "near = 1e-4\n",
        "far = 1000\n",
        "def render_mesh(mesh_dict, cam_int, cam_ext, device):\n",
        "    '''\n",
        "    input:\n",
        "        mesh: the output for construct_mesh function\n",
        "        cam_int: [b,3,3] normalized camera intrinsic matrix\n",
        "        cam_ext: [b,3,4] camera extrinsic matrix with the same scale as depth map\n",
        "        camera coord: x to right, z to front, y to down\n",
        "    \n",
        "    output:\n",
        "        render: [b,3,h,w]\n",
        "        disparity: [b,1,h,w]\n",
        "    '''\n",
        "    vertice = mesh_dict[\"vertice\"]  # [b,h*w,3]\n",
        "    faces = mesh_dict[\"faces\"]  # [b,nface,3]\n",
        "    attributes = mesh_dict[\"attributes\"]  # [b,h*w,4]\n",
        "    h, w = mesh_dict[\"size\"]\n",
        "\n",
        "    ############\n",
        "    # to NDC space\n",
        "    vertice_homo = lift_to_homo(vertice)  # [b,h*w,4]\n",
        "    # [b,1,3,4] x [b,h*w,4,1] = [b,h*w,3,1]\n",
        "    vertice_world = torch.matmul(cam_ext.unsqueeze(1), vertice_homo[..., None]).squeeze(-1)  # [b,h*w,3]\n",
        "    vertice_depth = vertice_world[..., -1:]  # [b,h*w,1]\n",
        "    attributes = torch.cat([attributes, vertice_depth], dim=-1)  # [b,h*w,5]\n",
        "    # [b,1,3,3] x [b,h*w,3,1] = [b,h*w,3,1]\n",
        "    vertice_world_homo = lift_to_homo(vertice_world)\n",
        "    persp = get_perspective_from_intrinsic(cam_int, near, far)  # [b,4,4]\n",
        "\n",
        "    # [b,1,4,4] x [b,h*w,4,1] = [b,h*w,4,1]\n",
        "    vertice_ndc = torch.matmul(persp.unsqueeze(1), vertice_world_homo[..., None]).squeeze(-1)  # [b,h*w,4]\n",
        "    vertice_ndc = vertice_ndc[..., :-1] / vertice_ndc[..., -1:]\n",
        "    vertice_ndc[..., :-1] *= -1\n",
        "    vertice_ndc[..., 0] *= w / h\n",
        "    print(\"vertices shape\", vertice_ndc.shape)\n",
        "    ############\n",
        "    # render\n",
        "    mesh = Meshes(vertice_ndc, faces)\n",
        "    pix_to_face, _, bary_coords, _ = rasterize_meshes(mesh, (h, w), faces_per_pixel=1, blur_radius=1e-6)  # [b,h,w,1] [b,h,w,1,3]\n",
        "\n",
        "    b, nf, _ = faces.size()\n",
        "    faces = faces.reshape(b, nf * 3, 1).repeat(1, 1, 5)  # [b,3f,5]\n",
        "    face_attributes = torch.gather(attributes, dim=1, index=faces)  # [b,3f,5]\n",
        "    face_attributes = face_attributes.reshape(b * nf, 3, 5)\n",
        "    output = interpolate_face_attributes(pix_to_face, bary_coords, face_attributes)\n",
        "    output = output.squeeze(-2).permute(0, 3, 1, 2)\n",
        "\n",
        "    render = output[:, :3]\n",
        "    mask = output[:, 3:4]\n",
        "    disparity = torch.reciprocal(output[:, 4:] + eps)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(disparity.reshape(1,h,w,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "    plt.title('final inverse_depth')\n",
        "    plt.colorbar()\n",
        "    return render, disparity, mask    \n",
        "    # return render * mask, disparity * mask, mask\n",
        "\n",
        "\n",
        "def construct_mesh(image, depth, cam_int, device):\n",
        "    '''\n",
        "    input:\n",
        "        rgbd: [b,4,h,w]\n",
        "            the first 3 channels for RGB\n",
        "            the last channel for normalized disparity, in range [0,1]\n",
        "        cam_int: [b,3,3] normalized camera intrinsic matrix\n",
        "    \n",
        "    output:\n",
        "        mesh_dict: define mesh in camera space, includes the following keys\n",
        "            vertice: [b,h*w,3]\n",
        "            faces:  [b,nface,3]\n",
        "            attributes: [b,h*w,c] include color and mask\n",
        "    '''\n",
        "    \n",
        "    b, c, h, w = image.shape\n",
        "    image = image.to(device)\n",
        "    depth = depth.to(device)\n",
        "    cam_int = cam_int.to(device)\n",
        "    depth = depth.permute(0, 2, 3, 1) \n",
        "    image = image.permute(0, 2, 3, 1)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(depth.reshape(1,h,w,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "    plt.title('gt depth')\n",
        "    plt.colorbar()\n",
        "    disparity = torch.reciprocal(depth + eps)  # [b,h,w,1]\n",
        "    disparity = (disparity - disparity.min()) / (disparity.max() - disparity.min())\n",
        "    depth = torch.reciprocal(disparity + eps)  # [b,h,w,1]\n",
        "    # print(\"\\nCamera Intrinsics K:\")\n",
        "    # print(np.array2string(cam_int.cpu().numpy(), precision=3, suppress_small=False))\n",
        "    ############\n",
        "    # get pixel coordinates\n",
        "    pixel_2d = get_screen_pixel_coord(h, w)  # [1,h,w,2]\n",
        "    pixel_2d_homo = lift_to_homo(pixel_2d)  # [1,h,w,3]\n",
        "\n",
        "    ############\n",
        "    # project pixels to 3D space\n",
        "    cam_int_inv = torch.inverse(cam_int)  # [b,3,3]\n",
        "    # print(\"\\nInverse Camera Intrinsics K^-1:\")\n",
        "    # print(np.array2string(K.cpu().numpy(), precision=3, suppress_small=False))\n",
        "    # [b,1,1,3,3] x [1,h,w,3,1] = [b,h,w,3,1]\n",
        "\n",
        "    pixel_3d = torch.matmul(cam_int_inv[:, None, None, :, :], pixel_2d_homo[..., None]).squeeze(-1)  # [b,h,w,3]\n",
        "    \n",
        "    pixel_3d = pixel_3d * depth  # [b,h,w,3]\n",
        "    vertice = pixel_3d.reshape(b, h * w, 3)  # [b,h*w,3]\n",
        "\n",
        "    ############\n",
        "    # construct faces\n",
        "    faces = get_faces(h, w).to(device)  # [1,nface,3]\n",
        "    faces = faces.repeat(b, 1, 1)  # [b,nface,3]\n",
        "\n",
        "    ############\n",
        "    # compute attributes\n",
        "    \n",
        "    attr_color = image.reshape(b, h * w, 3)  # [b,h*w,3]\n",
        "    attr_mask = get_visible_mask(disparity).reshape(b, h * w, 1)  # [b,h*w,1]\n",
        "    attr = torch.cat([attr_color, attr_mask], dim=-1)  # [b,h*w,4]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(disparity.reshape(1,h,w,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "    plt.title('inverse_depth')\n",
        "    plt.colorbar()\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(attr_mask.reshape(1,h,w,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "    plt.title('attribute mask')\n",
        "    plt.colorbar()\n",
        "    mesh_dict = {\n",
        "        \"vertice\": vertice,\n",
        "        \"faces\": faces,\n",
        "        \"attributes\": attr,\n",
        "        \"size\": [h, w],\n",
        "    }\n",
        "    return mesh_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow\n",
        "# Ground truth depthの読み込み\n",
        "exr_path = \"/home/rintoyagawa/ssd2/Code/invdepth/data/depths/debugdepth.exr\"\n",
        "z_min, z_max = 0.4, 1000\n",
        "\n",
        "def norm_depth(\n",
        "    depth: np.ndarray, min: float = 0.0, max: float = 1.0\n",
        ") -> np.ndarray:\n",
        "    return (depth - min) / (max - min)\n",
        "\n",
        "def denorm_depth(\n",
        "    depth: np.ndarray, min: float = 0.0, max: float = 1.0\n",
        ") -> np.ndarray:\n",
        "    return depth * (max - min) + min\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rA2p4ycCsfQ",
        "outputId": "884fc75b-e05e-4984-b1da-6d57ef75afd8"
      },
      "outputs": [],
      "source": [
        "def warp_with_inverse_depth_mesh(image, depth, device, K, RT_ref_to_view):\n",
        "    \"\"\"\n",
        "    スケーリングを改善したワーピング関数\n",
        "    \"\"\"\n",
        "    # 入力テンソルの形状を統一\n",
        "    K = torch.tensor(K, dtype=torch.float32, device=device)\n",
        "    RT_ref_to_view = torch.tensor(RT_ref_to_view, dtype=torch.float32, device=device)\n",
        "    if len(image.shape) == 2:\n",
        "        image = image.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    if len(K.shape) == 2:\n",
        "        K = K.unsqueeze(0)\n",
        "    if len(RT_ref_to_view.shape) == 2:\n",
        "        RT_ref_to_view = RT_ref_to_view.unsqueeze(0)\n",
        "\n",
        "    h, w = image.shape[-2:]\n",
        "\n",
        "    # safety_factor = 1e-4\n",
        "    # disp = 1 / depth + safety_factor\n",
        "\n",
        "    # warp to a random novel view\n",
        "    mesh = construct_mesh(image, depth, K, device)\n",
        "    warp_image, warp_disp, warp_mask = render_mesh(mesh, K, RT_ref_to_view, device)\n",
        "    # warped_image, mask = warp_with_projection(\n",
        "    #         image, depth, K, K, RT_ref_to_view, device)\n",
        "    return warp_image, warp_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfPgHrDvDA65",
        "outputId": "3e4f67ca-1b10-4245-bd89-b7f1d2afb8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# results\n",
        "# Load images\n",
        "\n",
        "img0, img1, img2 = load_images(\"/home/rintoyagawa/ssd2/Code/invdepth/data/images/debugleft.png\"\n",
        ", \"/home/rintoyagawa/ssd2/Code/invdepth/data/images/debugright.png\",\n",
        "                                \"/home/rintoyagawa/ssd2/Code/invdepth/data/images/debugright.png\")\n",
        "# img0, img1, img2 = load_images(\"/home/rintoyagawa/ssd2/Code/invde.png\"\n",
        "# , \"/home/rintoyagawa/ssd2/Code/invdepth/data/images/wood_mbs_dataB20_v1.png\",\n",
        "#                                 \"/home/rintoyagawa/ssd2/Code/invdepth/data/images/wood_mbs_dataB20_vpth/data/images/wood_mbs_dataB20_v02.png\")\n",
        "scene_depth = 1.65  # meters, adjust based on your scene\n",
        "# Blender settings\n",
        "interocular_distance = 0.02  # meters, adjust based on your Blender setting\n",
        "focal_length_m = 0.055  # adjust based on your Blender camera setting\n",
        "sensor_width_m = 0.024  # adjust based on your Blender camera setting\n",
        "\"\"\"\n",
        "for disparity\n",
        "base1 = interocular_distance\n",
        "base2 = interocular_distance * 2\n",
        "\"\"\"\n",
        "use_meshrenderer = True\n",
        "print(img0.shape)\n",
        "b, c, h, w = img0.shape\n",
        "# ref_params = {\n",
        "#     \"location\": [-68.599, -150.685, 119.469],\n",
        "#     \"rotation\": [84.0, 0.0, 226.0]}\n",
        "# target_params_1 = {\n",
        "#     \"location\": [-68.606, -150.692, 119.469],\n",
        "    # \"rotation\": [84.0, 0.0, 226.0]}\n",
        "ref_params = {\n",
        "    \"location\": [-0.0325, 0.0, 0.0],\n",
        "    \"rotation\": [1.57, 0.0, 0.0]}\n",
        "target_params_1 = {\n",
        "    \"location\": [0.0325, 0.0, 0.0],\n",
        "    \"rotation\": [1.57, 0.0, 0.0]}\n",
        "target_params_2 = {\n",
        "    \"location\": [-68.613, -150.699, 119.469],\n",
        "    \"rotation\": [84.0, 0.0, 226.0]}\n",
        "# results = train_inverse_depth(img0, img1, img2, focal_length_m, sensor_width_m, use_meshrenderer, device, locations=[ref_params['location'], target_params_1['location'], target_params_2['location']], rotations=[ref_params['rotation'], target_params_1['rotation'], target_params_2['rotation']], use_patch_loss=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depth values range [0.4, 1000]: [2.267578125, inf].\n",
            "Depth values out of range [0.4, 1000]: [2.267578125, inf]. Clipping.\n",
            "torch.Size([1, 1, 256, 256])\n",
            "tensor(0.0019) tensor(1.)\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 6.50000000e-02]\n",
            " [0.00000000e+00 1.00000000e+00 4.95874875e-18 0.00000000e+00]\n",
            " [0.00000000e+00 1.06297301e-18 1.00000000e+00 0.00000000e+00]]\n",
            "[[2.29, 0.0, 0.5], [0.0, 2.29, 0.5], [0.0, 0.0, 1.0]]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[1, 65536, 3]' is invalid for input of size 262144",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(RT_ref_to_1)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(K)\n\u001b[0;32m---> 40\u001b[0m pred_img1, mask1 \u001b[38;5;241m=\u001b[39m \u001b[43mwarp_with_inverse_depth_mesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43mimg0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43mgt_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43mRT_ref_to_1\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pred_img1, mask1 = warp_with_inverse_depth_projection(\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# img0,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# gt_depth,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# RT_ref_to_1\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     55\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
            "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mwarp_with_inverse_depth_mesh\u001b[0;34m(image, depth, device, K, RT_ref_to_view)\u001b[0m\n\u001b[1;32m     16\u001b[0m h, w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# safety_factor = 1e-4\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# disp = 1 / depth + safety_factor\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# warp to a random novel view\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m warp_image, warp_disp, warp_mask \u001b[38;5;241m=\u001b[39m render_mesh(mesh, K, RT_ref_to_view, device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# warped_image, mask = warp_with_projection(\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         image, depth, K, K, RT_ref_to_view, device)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 116\u001b[0m, in \u001b[0;36mconstruct_mesh\u001b[0;34m(image, depth, cam_int, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m faces \u001b[38;5;241m=\u001b[39m faces\u001b[38;5;241m.\u001b[39mrepeat(b, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [b,nface,3]\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m############\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# compute attributes\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m attr_color \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [b,h*w,3]\u001b[39;00m\n\u001b[1;32m    117\u001b[0m attr_mask \u001b[38;5;241m=\u001b[39m get_visible_mask(disparity)\u001b[38;5;241m.\u001b[39mreshape(b, h \u001b[38;5;241m*\u001b[39m w, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [b,h*w,1]\u001b[39;00m\n\u001b[1;32m    118\u001b[0m attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([attr_color, attr_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [b,h*w,4]\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 65536, 3]' is invalid for input of size 262144"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import OpenEXR\n",
        "exr_file = OpenEXR.InputFile(exr_path)\n",
        "dw = exr_file.header()[\"dataWindow\"]\n",
        "size = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)\n",
        "channel_keys = sorted(exr_file.header()[\"channels\"].keys())\n",
        "\n",
        "if channel_keys == sorted([\"R\", \"G\", \"B\"]):\n",
        "    (v, _, _) = exr_file.channels(\"RGB\")\n",
        "    gt_depth = np.frombuffer(v, dtype=np.float16)\n",
        "elif channel_keys == sorted([\"V\"]):\n",
        "    v = exr_file.channels(\"V\")[0]\n",
        "    gt_depth = np.frombuffer(v, dtype=np.float32).astype(np.float16)\n",
        "\n",
        "gt_depth = gt_depth.reshape(size[1], size[0])\n",
        "print(\n",
        "        f\"Depth values range [{z_min}, {z_max}]: [{gt_depth.min()}, {gt_depth.max()}].\"\n",
        "    )\n",
        "if gt_depth.min() < z_min or gt_depth.max() > z_max:\n",
        "    print(\n",
        "        f\"Depth values out of range [{z_min}, {z_max}]: [{gt_depth.min()}, {gt_depth.max()}]. Clipping.\"\n",
        "    )\n",
        "# gt_depth = (gt_depth.clip(z_min, z_max))\n",
        "gt_depth = norm_depth(gt_depth.clip(z_min, z_max), z_min, z_max)\n",
        "gt_depth = torch.tensor(gt_depth).float().unsqueeze(0).unsqueeze(0)\n",
        "print(gt_depth.shape)\n",
        "print(gt_depth.min(), gt_depth.max())\n",
        "K, RT_ref = get_camera_matrices(w, h, focal_length_m, sensor_width_m,ref_params['location'], ref_params['rotation'])\n",
        "_, RT1 = get_camera_matrices(w, h, focal_length_m, sensor_width_m, target_params_1['location'], target_params_1['rotation'])\n",
        "_, RT2 = get_camera_matrices(w, h, focal_length_m, sensor_width_m, target_params_2['location'], target_params_2['rotation'])\n",
        "RT_ref_to_1 = compute_relative_transform(RT_ref, RT1)\n",
        "RT_ref_to_2 = compute_relative_transform(RT_ref, RT2)\n",
        "# RT_ref_to_1 = [[1.0000, 0.0000, 0.0000, -0.07],\n",
        "#                 [0.0000, 1.0000, 0.0000, 0.07],\n",
        "#                 [0.0000, 0.0000, 1.0000, 0.0000]]   \n",
        "K = [[2.2900, 0.0000, 0.5000],\n",
        "    [0.0000, 2.2900, 0.5000],\n",
        "    [0.0000, 0.0000, 1.0000]]\n",
        "print(RT_ref_to_1)\n",
        "print(K)\n",
        "pred_img1, mask1 = warp_with_inverse_depth_mesh(\n",
        "img0,\n",
        "gt_depth,\n",
        "device,\n",
        "K,\n",
        "RT_ref_to_1\n",
        ")\n",
        "# pred_img1, mask1 = warp_with_inverse_depth_projection(\n",
        "# img0,\n",
        "# gt_depth,\n",
        "# device,\n",
        "# K,\n",
        "# RT_ref_to_1\n",
        "# )\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "plt.imshow(img1.permute(0,2,3,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "plt.title('Ground Truth Image 1')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(pred_img1.permute(0,2,3,1).squeeze(0).cpu().detach().numpy()) #, cmap='gray') # Call detach() before converting to NumPy array.\n",
        "plt.title('Predicted Image 1')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
